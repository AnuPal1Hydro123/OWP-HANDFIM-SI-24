{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b09538d-e009-470c-9575-d43dbc34a676",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4 netCDF4 geopandas shapely numpy\n",
    "!pip install netCDF4 numpy\n",
    "!pip install netCDF4 geopandas shapely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827c0f65-245e-439f-a4fc-bf296eb4d1ba",
   "metadata": {},
   "source": [
    "## Acess Key for CIROH s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f348f67-b106-494c-828e-8b932c3c36f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Will be provided upon request\n",
    "\n",
    "import os\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = '----'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = '-----'\n",
    "os.environ['AWS_DEFAULT_REGION'] = '-----'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c218af8-2343-4a59-aacd-caee6971370e",
   "metadata": {},
   "source": [
    "## List the contents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758bc64a-246c-41b6-aeb7-7a8a24a8ac59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 ls s3://ciroh-owp-hand-fim/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ec8bad-baab-4ce3-9287-47ea9d8177c6",
   "metadata": {},
   "source": [
    "# Downloading the HAND Rasters and FIM Hydrofabric for Specific AOI for HAND FIM v-4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b840d5-30e1-4a45-9186-de3f0a6a7423",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 sync s3://ciroh-owp-hand-fim/07080101 07080101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc9948-dbff-4c92-9355-a3f3edaaa32d",
   "metadata": {},
   "source": [
    "## Cloning the FIM script from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d769517b-d291-4abc-9688-a52c6b470d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git --version\n",
    "\n",
    "!git clone https://github.com/NOAA-OWP/inundation-mapping.git inundation-mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3652408d-1e02-4671-82ea-23e6ee07f1f8",
   "metadata": {},
   "source": [
    "## Creating the feature_id file from hydrotable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd66b8d0-6bd5-4bdb-a3fd-835513df2abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the hydrotable.csv file from AOI\n",
    "hydrotable_path = r'07080101/hydrotable.csv'\n",
    "hydrotable_df = pd.read_csv(hydrotable_path)\n",
    "\n",
    "# Get unique feature IDs\n",
    "unique_feature_ids = hydrotable_df['feature_id'].drop_duplicates()\n",
    "\n",
    "# Create a DataFrame with the unique feature IDs\n",
    "unique_feature_ids_df = pd.DataFrame(unique_feature_ids, columns=['feature_id'])\n",
    "\n",
    "# Add a new column with row numbers starting from zero\n",
    "unique_feature_ids_df.insert(0, '', range(len(unique_feature_ids_df)))\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_csv_path = r'07080101_feature_id.csv'\n",
    "unique_feature_ids_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "\n",
    "#print(unique_feature_ids_df.head())\n",
    "print('Row numbers added and CSV file saved successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349700e0-9358-4be5-af91-3858a753d95c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install xarray zarr fsspec s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2023cb80-6a21-4e77-a632-3543f88cd86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "out_dir='07080101_run'\n",
    "os.makedirs(out_dir,exist_ok=True)\n",
    "fid_07080101=pd.read_csv('07080101_feature_id.csv')\n",
    "print(fid_07080101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004b9928-da58-46dc-87c7-b4df7083725f",
   "metadata": {},
   "source": [
    "## Short Range forecasting data for the AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae48e70-e420-405f-9c89-8a66a99631ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import netCDF4 as nc\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "# Define directories\n",
    "os.makedirs(\"Forecasted_streamflow\", exist_ok=True)\n",
    "\n",
    "url_base = \"https://nomads.ncep.noaa.gov/pub/data/nccf/com/nwm/prod/\"\n",
    "output_dir = \"07080101_run\"\n",
    "download_dir = \"Forecasted_streamflow\"\n",
    "output_csv_filename = \"07080101_feature_id.csv\"\n",
    "shutil.copy(output_csv_filename, output_dir)\n",
    "\n",
    "def clear_download_directory(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "\n",
    "def download_nc_files(date_str, current_hour):\n",
    "    url = f\"{url_base}/nwm.{date_str}/short_range/\"\n",
    "    date_output_dir = os.path.join(download_dir, date_str)\n",
    "    os.makedirs(date_output_dir, exist_ok=True)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    pattern = re.compile(rf'nwm\\.t{current_hour:02d}z\\.short_range\\.channel_rt\\.f\\d{{3}}\\.conus\\.nc')\n",
    "    nc_files = [link['href'] for link in soup.find_all('a', href=True) if pattern.search(link['href'])]\n",
    "\n",
    "    if not nc_files:\n",
    "        return False, date_output_dir\n",
    "\n",
    "    hour_output_dir = os.path.join(date_output_dir, f'{current_hour:02d}')\n",
    "    os.makedirs(hour_output_dir, exist_ok=True)\n",
    "\n",
    "    for nc_file in nc_files:\n",
    "        file_url = url + nc_file\n",
    "        file_path = os.path.join(hour_output_dir, nc_file)\n",
    "        \n",
    "        print(f'Downloading {file_url} to {file_path}')\n",
    "        file_response = requests.get(file_url)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(file_response.content)\n",
    "        print(f'Download completed for {file_path}')\n",
    "\n",
    "    print('All downloads completed for the date:', date_str)\n",
    "    return True, hour_output_dir\n",
    "\n",
    "def process_netcdf_file(netcdf_file_path, filter_df, output_folder_path):\n",
    "    base_filename = os.path.basename(netcdf_file_path).replace('.nc', '')\n",
    "    output_csv_file_path = os.path.join(output_folder_path, f'{base_filename}.csv')\n",
    "\n",
    "    try:\n",
    "        ds = nc.Dataset(netcdf_file_path, 'r')\n",
    "        streamflow_data = ds.variables['streamflow'][:]\n",
    "        feature_ids = ds.variables['feature_id'][:]\n",
    "        ds.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading NetCDF file {netcdf_file_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    if len(streamflow_data) == 0 or len(feature_ids) == 0:\n",
    "        print(f\"No data found in {netcdf_file_path}\")\n",
    "        return\n",
    "\n",
    "    data_df = pd.DataFrame({\n",
    "        'feature_id': feature_ids,\n",
    "        'discharge': streamflow_data\n",
    "    })\n",
    "\n",
    "    filtered_df = data_df[data_df['feature_id'].isin(filter_df['feature_id'])]\n",
    "    merged_df = pd.merge(filter_df[['feature_id']], filtered_df, on='feature_id')\n",
    "    merged_df.to_csv(output_csv_file_path, index=False)\n",
    "    print(f'Filtered DataFrame saved to {output_csv_file_path}')\n",
    "\n",
    "def main():\n",
    "    # Clear download directory before downloading new files\n",
    "    clear_download_directory(download_dir)\n",
    "\n",
    "    today = datetime.utcnow().strftime('%Y%m%d')\n",
    "    current_hour = datetime.utcnow().hour\n",
    "\n",
    "    success = False\n",
    "    attempts = 0\n",
    "\n",
    "    while not success and attempts < 24:\n",
    "        attempts += 1\n",
    "        success, date_output_dir = download_nc_files(today, current_hour)\n",
    "        if not success:\n",
    "            current_hour = (current_hour - 1) % 24\n",
    "            if current_hour == 23:\n",
    "                today = (datetime.utcnow() - timedelta(days=1)).strftime('%Y%m%d')\n",
    "\n",
    "    if not success:\n",
    "        print(\"No recent forecast data found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    filter_csv_file_path = os.path.join(output_dir, output_csv_filename)\n",
    "    output_folder_path = os.path.join(download_dir, \"Data\")\n",
    "    os.makedirs(output_folder_path, exist_ok=True)\n",
    "\n",
    "    filter_df = pd.read_csv(filter_csv_file_path)\n",
    "\n",
    "    if os.path.exists(date_output_dir):\n",
    "        for root, _, files in os.walk(date_output_dir):\n",
    "            for filename in files:\n",
    "                if filename.endswith('.nc'):\n",
    "                    netcdf_file_path = os.path.join(root, filename)\n",
    "                    process_netcdf_file(netcdf_file_path, filter_df, output_folder_path)\n",
    "    \n",
    "    csv_directory = output_folder_path\n",
    "    csv_files = [file for file in os.listdir(csv_directory) if file.endswith('.csv')]\n",
    "\n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found after processing NetCDF files.\")\n",
    "        return\n",
    "\n",
    "    combined_df = pd.concat([pd.read_csv(os.path.join(csv_directory, file))[['feature_id', 'discharge']] for file in csv_files])\n",
    "\n",
    "    combined_df = combined_df.pivot_table(index='feature_id', values='discharge', aggfunc=list).apply(pd.Series.explode).reset_index()\n",
    "    combined_df['discharge'] = combined_df['discharge'].astype(float)\n",
    "    combined_df = combined_df.groupby('feature_id')['discharge'].apply(list).reset_index()\n",
    "    for i in range(1, len(combined_df['discharge'][0]) + 1):\n",
    "        combined_df[f'discharge_{i}'] = combined_df['discharge'].apply(lambda x: x[i-1] if i-1 < len(x) else None)\n",
    "    combined_df.drop(columns=['discharge'], inplace=True)\n",
    "\n",
    "    output_file = os.path.join(download_dir, \"combined_streamflow.csv\")\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "    # Extract maximum discharge for each feature_id\n",
    "    max_discharge_df = combined_df.set_index('feature_id').max(axis=1).reset_index()\n",
    "    max_discharge_df.columns = ['feature_id', 'discharge']\n",
    "\n",
    "    max_output_file = os.path.join(download_dir, \"forecasted_discharge.csv\")\n",
    "    max_discharge_df.to_csv(max_output_file, index=False)\n",
    "\n",
    "    print(f'Maximum discharge values saved to {max_output_file}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309e2526-5fbe-4638-aec7-8ff97a520143",
   "metadata": {},
   "source": [
    "## Downloading NWM retrospective for all feature_ids to the directory with the new code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b809f6b-d6a9-4e58-a517-1be9bdce999a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#install these libraries if they aren't already installed\n",
    "!pip install zarr\n",
    "!pip install xarray\n",
    "!pip install s3fs\n",
    "!pip install numpy\n",
    " \n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import s3fs\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# open the zarr store\n",
    "# url = \"s3://noaa-nwm-retrospective-2-1-zarr-pds/chrtout.zarr\"\n",
    "url = \"s3://noaa-nwm-retrospective-3-0-pds/CONUS/zarr/chrtout.zarr\"\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "store = xr.open_zarr(s3fs.S3Map(url, s3=fs))\n",
    "\n",
    "store\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Define the output directory\n",
    "output_directory = '07080101_run'  # Change this to your desired directory\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Function to get the time series for a specified reach id and time range\n",
    "# then write it out to a csv file.\n",
    "def GetAndWriteTimeSeriesAtReach(reach_id, start_time_index, end_time_index):\n",
    "    # Filter the streamflow array for the specific reach_id and time range\n",
    "    flows = streamflow_array.where(feature_id_array == reach_id, drop=True)\n",
    "    df_flows = flows[start_time_index:end_time_index].to_dataframe()\n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_filename = os.path.join(output_directory, f'{reach_id}.csv')\n",
    "    df_flows.to_csv(csv_filename)\n",
    "    \n",
    "    #print(f\"Saved CSV file: {csv_filename}\")\n",
    "\n",
    "# Get the xarray array of the various values\n",
    "time_array = store['time']\n",
    "feature_id_array = store['feature_id']\n",
    "streamflow_array = store['streamflow']\n",
    "\n",
    "# Define the feature IDs to check for\n",
    "feature_id_df = pd.read_csv('07080101_feature_id.csv')\n",
    "feature_ids = feature_id_df['feature_id'].tolist()  # Assuming the column name is 'feature_id'\n",
    "\n",
    "# Specify the start and end times of interest  24hr format\n",
    "start_time = datetime(2015, 5, 23, 0, 0, 0)\n",
    "end_time = datetime(2015, 5, 23, 4, 0, 0)\n",
    "\n",
    "# Get the indices for the needed dates\n",
    "zero_start_time = datetime(1979, 2, 1, 0, 0, 0)\n",
    "start_time_index = int((start_time - zero_start_time).total_seconds() / 3600)\n",
    "end_time_index = int((end_time - zero_start_time).total_seconds() / 3600)\n",
    "\n",
    "# Loop over each feature ID from the CSV file, get the time series, and print it\n",
    "for reach_id in feature_ids:\n",
    "    GetAndWriteTimeSeriesAtReach(reach_id, start_time_index, end_time_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4fb902-9c50-483c-8915-ca8e07af8779",
   "metadata": {},
   "source": [
    "## Converting Streamflow data into HAND FIM input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "221328c5-0e5f-4400-9878-dc92e805794f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 07080101_23may.csv - missing required columns: {'time', 'streamflow'}\n",
      "Skipping concatenated_output.csv - missing required columns: {'feature_id', 'streamflow'}\n",
      "Skipping combined_flow.csv - missing required columns: {'time', 'feature_id', 'streamflow'}\n",
      "Skipping 07080101_23may_2PM.csv - missing required columns: {'time', 'streamflow'}\n",
      "                  time  14805783  14803879  14802695  14805583  14805111  \\\n",
      "0  2015-05-23 01:00:00      0.01      0.01      0.07      0.08       0.0   \n",
      "1  2015-05-23 02:00:00      0.01      0.01      0.07      0.08       0.0   \n",
      "2  2015-05-23 03:00:00      0.01      0.01      0.07      0.08       0.0   \n",
      "3  2015-05-23 04:00:00      0.01      0.01      0.07      0.08       0.0   \n",
      "\n",
      "      14806061  14806671  14804565  14803051  ...  14803713  14804179  \\\n",
      "0  1716.849962      0.01      0.84       0.0  ...       0.0       0.0   \n",
      "1  1715.559962      0.01      0.83       0.0  ...       0.0       0.0   \n",
      "2  1714.259962      0.01      0.83       0.0  ...       0.0       0.0   \n",
      "3  1712.939962      0.01      0.83       0.0  ...       0.0       0.0   \n",
      "\n",
      "   14803333  14805701  14805391     14809463  14806221  14806747  14803569  \\\n",
      "0      0.01      0.01       0.0  1368.269969      0.05      0.09      0.18   \n",
      "1      0.01      0.01       0.0  1367.819969      0.05      0.09      0.18   \n",
      "2      0.01      0.01       0.0  1367.369969      0.05      0.09      0.18   \n",
      "3      0.01      0.01       0.0  1366.929969      0.05      0.09      0.18   \n",
      "\n",
      "   14805713  \n",
      "0      0.01  \n",
      "1      0.01  \n",
      "2      0.01  \n",
      "3      0.01  \n",
      "\n",
      "[4 rows x 1195 columns]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the date and time in the format 'yyyy-mm-dd HH:MM:SS':  2015-05-23 02:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time        2015-05-23 02:00:00\n",
      "14805783                   0.01\n",
      "14803879                   0.01\n",
      "14802695                   0.07\n",
      "14805583                   0.08\n",
      "                   ...         \n",
      "14809463            1367.819969\n",
      "14806221                   0.05\n",
      "14806747                   0.09\n",
      "14803569                   0.18\n",
      "14805713                   0.01\n",
      "Name: 2015-05-23 02:00:00, Length: 1195, dtype: object\n",
      "Transposed output saved to 07080101_run/07080101_23may_2PM.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# List to store pivoted DataFrames\n",
    "pivoted_dfs = []\n",
    "\n",
    "output_directory = '07080101_run'\n",
    "\n",
    "for filename in os.listdir(output_directory):\n",
    "    if filename.endswith('.csv'):\n",
    "\n",
    "        file_path = os.path.join(output_directory, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        required_columns = {'time', 'feature_id', 'streamflow'}\n",
    "        if required_columns.issubset(df.columns):\n",
    "            # Keep only the 'time', 'feature_id', and 'streamflow' columns\n",
    "            df = df[['time', 'feature_id', 'streamflow']]\n",
    "            \n",
    "            # Pivot the DataFrame\n",
    "            df_pivoted = df.pivot(index='time', columns='feature_id', values='streamflow')\n",
    "            \n",
    "            # Remove the 'feature_id' header from the columns\n",
    "            df_pivoted.columns.name = None\n",
    "            \n",
    "            # Append the pivoted DataFrame to the list\n",
    "            pivoted_dfs.append(df_pivoted)\n",
    "        else:\n",
    "            # Print warning and skip this file\n",
    "            print(f\"Skipping {filename} - missing required columns: {required_columns - set(df.columns)}\")\n",
    "\n",
    "\n",
    "if pivoted_dfs:\n",
    "    final_df = pd.concat(pivoted_dfs, axis=1)\n",
    "    \n",
    "\n",
    "    final_df = final_df.reset_index()\n",
    "    print(final_df)\n",
    "    output_file_path = os.path.join(output_directory,'concatenated_output.csv')\n",
    "    final_df.to_csv(output_file_path, index=False)\n",
    "     \n",
    "    xyz_df = pd.read_csv(output_file_path)\n",
    "\n",
    "    date_time_input = input(\"Enter the date and time in the format 'yyyy-mm-dd HH:MM:SS': \")\n",
    "\n",
    "    date_time_index = xyz_df.index[xyz_df['time'] == date_time_input].tolist()\n",
    "\n",
    "    if date_time_index:\n",
    "        \n",
    "        transposed_row = xyz_df.iloc[date_time_index[0]].transpose()\n",
    "        transposed_row.name = date_time_input  # Set the name of the Series to the input date and time\n",
    "        print(transposed_row)\n",
    "    else:\n",
    "        print(\"No matching date and time found.\")\n",
    "        \n",
    "    transposed_row_df = transposed_row.to_frame().reset_index()\n",
    "    transposed_row_df.columns = ['feature_id', 'discharge']  # Rename columns\n",
    "    transposed_row_df = transposed_row_df.drop(index=0).reset_index(drop=True)  # Drop the 'time' row\n",
    "\n",
    "    # Save the transposed DataFrame to a CSV file\n",
    "    transposed_output_file_path = os.path.join(output_directory, '07080101_23may_2PM.csv')\n",
    "    transposed_row_df.to_csv(transposed_output_file_path, index=False)\n",
    "\n",
    "    print(f\"Transposed output saved to {transposed_output_file_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid files were processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c72fb890-e25f-49a8-b03f-8d3fdc6a7a5f",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "Requirement already satisfied: numba in /srv/conda/envs/notebook/lib/python3.11/site-packages (0.58.1)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from numba) (0.41.1)\n",
      "Requirement already satisfied: numpy<1.27,>=1.22 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from numba) (1.26.4)\n",
      "Requirement already satisfied: shapely in /srv/conda/envs/notebook/lib/python3.11/site-packages (2.0.3)\n",
      "Collecting pyogrio\n",
      "  Using cached pyogrio-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.14 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from shapely) (1.26.4)\n",
      "Requirement already satisfied: certifi in /srv/conda/envs/notebook/lib/python3.11/site-packages (from pyogrio) (2024.2.2)\n",
      "Requirement already satisfied: packaging in /srv/conda/envs/notebook/lib/python3.11/site-packages (from pyogrio) (24.0)\n",
      "Using cached pyogrio-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.3 MB)\n",
      "Installing collected packages: pyogrio\n",
      "Successfully installed pyogrio-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install numba\n",
    "!pip install shapely pyogrio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cf9e49-17b4-455c-9155-579331e64cda",
   "metadata": {},
   "source": [
    "## Virtual .env file to replace docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "562d968b-6626-4b4b-87f9-cc97205a69f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /home/jovyan\n",
      ".env file created at /home/jovyan/inundation-mapping/.env with content:\n",
      "\n",
      "inputsDir=inputs\n",
      "outputsDir=output\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "# Define the relative path to the .env file from the current working directory\n",
    "env_file_path = \"inundation-mapping/.env\"  # Go up one level to the inundation-mapping directory\n",
    "\n",
    "# Define the content for the .env file\n",
    "env_content = \"\"\"\n",
    "inputsDir=inputs\n",
    "outputsDir=output\n",
    "\"\"\"\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(env_file_path), exist_ok=True)\n",
    "\n",
    "# Write the content to the .env file\n",
    "with open(env_file_path, \"w\") as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "print(f\".env file created at {os.path.abspath(env_file_path)} with content:\\n{env_content}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f51e5b-8440-4a3e-bf02-35d1872b9cc9",
   "metadata": {},
   "source": [
    "## FIM maps with retrospective discharge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7a909b1-91a0-4044-bea0-c50e36e839dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied /home/jovyan/07080101 to /home/jovyan/output/07080101\n",
      "Copied first row of /home/jovyan/output/07080101/branch_ids.csv to /home/jovyan/output/fim_inputs.csv\n",
      "inputsDir: inputs\n",
      "outputsDir: output\n",
      "Completed in 0.37 minutes.\n",
      "\n",
      "Inundation mapping completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in cast\", category=RuntimeWarning)\n",
    "\n",
    "# Set up paths\n",
    "repository_path = \"/home/jovyan/inundation-mapping\"  # Update to the correct repository path\n",
    "tools_path = os.path.join(repository_path, \"tools\")\n",
    "src_path = os.path.join(repository_path, \"src\")\n",
    "output_path = \"/home/jovyan/output\"\n",
    "fim_temp_path = \"/home/jovyan/Outputs_temp\"\n",
    "data_path = \"/home/jovyan/07080101_run\"\n",
    "huc_folder_path = \"/home/jovyan/07080101\"\n",
    "output_huc_folder_path = os.path.join(output_path, \"07080101\")\n",
    "branch_ids_file = os.path.join(output_huc_folder_path, \"branch_ids.csv\")\n",
    "fim_inputs_file = os.path.join(output_path, \"fim_inputs.csv\")\n",
    "\n",
    "# Ensure all necessary directories exist\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "os.makedirs(fim_temp_path, exist_ok=True)\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "# Step 1: Copy the entire HUC folder to the output directory\n",
    "if os.path.exists(output_huc_folder_path):\n",
    "    shutil.rmtree(output_huc_folder_path)  # Remove the existing folder if it exists\n",
    "shutil.copytree(huc_folder_path, output_huc_folder_path)\n",
    "print(f\"Copied {huc_folder_path} to {output_huc_folder_path}\")\n",
    "\n",
    "# Step 2: Copy branch_ids.csv to the output folder, keep only the first row, and rename it to fim_inputs.csv\n",
    "if os.path.exists(branch_ids_file):\n",
    "    # Read the branch_ids.csv file\n",
    "    df = pd.read_csv(branch_ids_file)\n",
    "    \n",
    "    # Keep only the first row\n",
    "    #df_first_row = df.iloc[:0]\n",
    "    \n",
    "    # Save the first row to fim_inputs.csv\n",
    "    #df_first_row.to_csv(fim_inputs_file, index=False)\n",
    "    df.to_csv(fim_inputs_file, index=False)\n",
    "    print(f\"Copied first row of {branch_ids_file} to {fim_inputs_file}\")\n",
    "else:\n",
    "    print(f\"{branch_ids_file} does not exist\")\n",
    "\n",
    "# Change to the tools directory\n",
    "os.chdir(tools_path)\n",
    "\n",
    "# Load environment variables from .env file located in the inundation-mapping directory\n",
    "dotenv_path = os.path.join(repository_path, \".env\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Verify that the environment variables are loaded\n",
    "inputs_dir = os.getenv('inputsDir')\n",
    "outputs_dir = os.getenv('outputsDir')\n",
    "print(f\"inputsDir: {inputs_dir}\")\n",
    "print(f\"outputsDir: {outputs_dir}\")\n",
    "\n",
    "# Add src and repository_path to the Python path\n",
    "sys.path.append(src_path)\n",
    "sys.path.append(repository_path)\n",
    "\n",
    "# Define the command to run the inundation script\n",
    "command_to_run = [\n",
    "    sys.executable,\n",
    "    \"inundate_mosaic_wrapper.py\",\n",
    "    \"-y\", output_path,\n",
    "    \"-u\", \"07080101\",  # Replace with appropriate HUC\n",
    "    \"-f\", os.path.join(data_path, \"07080101_23may_2PM.csv\"),\n",
    "    \"-i\", os.path.join(output_huc_folder_path, \"inundation_retro.tif\"),\n",
    "    \"-d\", os.path.join(output_huc_folder_path, \"depth_retro.tif\")\n",
    "]\n",
    "\n",
    "# Run the command with the correct working directory and PYTHONPATH\n",
    "env = os.environ.copy()\n",
    "env['PYTHONPATH'] = f\"{src_path}{os.pathsep}{repository_path}\"\n",
    "\n",
    "result = subprocess.run(command_to_run, cwd=tools_path, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# Print the output and error (if any)\n",
    "print(result.stdout.decode())\n",
    "if result.stderr:\n",
    "    print(result.stderr.decode())\n",
    "\n",
    "# Check if the command was successful\n",
    "if result.returncode == 0:\n",
    "    print(\"Inundation mapping completed successfully.\")\n",
    "else:\n",
    "    print(\"Failed to complete inundation mapping.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13739e99-6c33-4551-a804-f280f1d37fdc",
   "metadata": {},
   "source": [
    "## FIM maps with forecasted streamflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2f1f08-51cc-424f-9a31-40149b0e59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in cast\", category=RuntimeWarning)\n",
    "\n",
    "# Set up paths\n",
    "repository_path = \"/home/jovyan/inundation-mapping\"  # Update to the correct repository path\n",
    "tools_path = os.path.join(repository_path, \"tools\")\n",
    "src_path = os.path.join(repository_path, \"src\")\n",
    "output_path = \"/home/jovyan/output\"\n",
    "fim_temp_path = \"/home/jovyan/Outputs_temp\"\n",
    "data_path = \"/home/jovyan/Forecasted_streamflow\"\n",
    "huc_folder_path = \"/home/jovyan/07080101\"\n",
    "output_huc_folder_path = os.path.join(output_path, \"07080101\")\n",
    "branch_ids_file = os.path.join(output_huc_folder_path, \"branch_ids.csv\")\n",
    "fim_inputs_file = os.path.join(output_path, \"fim_inputs.csv\")\n",
    "\n",
    "# Ensure all necessary directories exist\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "os.makedirs(fim_temp_path, exist_ok=True)\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "# Step 1: Copy the entire HUC folder to the output directory\n",
    "if os.path.exists(output_huc_folder_path):\n",
    "    shutil.rmtree(output_huc_folder_path)  # Remove the existing folder if it exists\n",
    "shutil.copytree(huc_folder_path, output_huc_folder_path)\n",
    "print(f\"Copied {huc_folder_path} to {output_huc_folder_path}\")\n",
    "\n",
    "# Step 2: Copy branch_ids.csv to the output folder, keep only the first row, and rename it to fim_inputs.csv\n",
    "if os.path.exists(branch_ids_file):\n",
    "    # Read the branch_ids.csv file\n",
    "    df = pd.read_csv(branch_ids_file)\n",
    "    \n",
    "    # Keep only the first row\n",
    "    #df_first_row = df.iloc[:0]\n",
    "    \n",
    "    # Save the first row to fim_inputs.csv\n",
    "    #df_first_row.to_csv(fim_inputs_file, index=False)\n",
    "    df.to_csv(fim_inputs_file, index=False)\n",
    "    print(f\"Copied first row of {branch_ids_file} to {fim_inputs_file}\")\n",
    "else:\n",
    "    print(f\"{branch_ids_file} does not exist\")\n",
    "\n",
    "# Change to the tools directory\n",
    "os.chdir(tools_path)\n",
    "\n",
    "# Load environment variables from .env file located in the inundation-mapping directory\n",
    "dotenv_path = os.path.join(repository_path, \".env\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Verify that the environment variables are loaded\n",
    "inputs_dir = os.getenv('inputsDir')\n",
    "outputs_dir = os.getenv('outputsDir')\n",
    "print(f\"inputsDir: {inputs_dir}\")\n",
    "print(f\"outputsDir: {outputs_dir}\")\n",
    "\n",
    "# Add src and repository_path to the Python path\n",
    "sys.path.append(src_path)\n",
    "sys.path.append(repository_path)\n",
    "\n",
    "# Define the command to run the inundation script\n",
    "command_to_run = [\n",
    "    sys.executable,\n",
    "    \"inundate_mosaic_wrapper.py\",\n",
    "    \"-y\", output_path,\n",
    "    \"-u\", \"07080101\",  # Replace with appropriate HUC\n",
    "    \"-f\", os.path.join(data_path, \"forecasted_discharge.csv\"),\n",
    "    \"-i\", os.path.join(output_huc_folder_path, \"inundation_forecasted.tif\"),\n",
    "    \"-d\", os.path.join(output_huc_folder_path, \"depth_forecasted.tif\")\n",
    "]\n",
    "\n",
    "# Run the command with the correct working directory and PYTHONPATH\n",
    "env = os.environ.copy()\n",
    "env['PYTHONPATH'] = f\"{src_path}{os.pathsep}{repository_path}\"\n",
    "\n",
    "result = subprocess.run(command_to_run, cwd=tools_path, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# Print the output and error (if any)\n",
    "print(result.stdout.decode())\n",
    "if result.stderr:\n",
    "    print(result.stderr.decode())\n",
    "\n",
    "# Check if the command was successful\n",
    "if result.returncode == 0:\n",
    "    print(\"Inundation mapping completed successfully.\")\n",
    "else:\n",
    "    print(\"Failed to complete inundation mapping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c3baeb-6b7e-4cc2-9d4e-b8937efa2e36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
